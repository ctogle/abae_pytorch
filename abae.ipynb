{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linecount(path):\n",
    "    j = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for l in f:\n",
    "            j += 1\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class dataloader:\n",
    "    \n",
    "    def __init__(self, w2i, path, batchsize=16, negsize=20, n_batches=None):\n",
    "        self.w2i = w2i\n",
    "        self.path = path\n",
    "        self.batchsize = batchsize\n",
    "        self.negsize = negsize\n",
    "        if not n_batches:\n",
    "            n_batches = (linecount(path) // batchsize) + 1\n",
    "        self.n_batches = n_batches\n",
    "        \n",
    "    def b2i(self, batch):\n",
    "        batch = [l.split() for l in batch]\n",
    "        lengths = [len(l) for l in batch]\n",
    "        index = np.zeros((len(batch), max(lengths)))\n",
    "        for j, (words, length) in enumerate(zip(batch, lengths)):\n",
    "            index[j, :length] = [(self.w2i[w] if w in self.w2i else self.w2i['<unk>']) for w in words]\n",
    "        return index\n",
    "\n",
    "    def __iter___AAAAA(self):\n",
    "        with open(self.path, 'r') as f:\n",
    "            i, j, batch = 0, 0, []\n",
    "            for l in f:\n",
    "                if j == self.batchsize:\n",
    "                    yield self.b2i(batch)\n",
    "                    j, batch = 1, [l]\n",
    "                    i += 1\n",
    "                    if i > self.n_batches:\n",
    "                        break\n",
    "                else:\n",
    "                    j += 1\n",
    "                    batch.append(l)\n",
    "            if batch:\n",
    "                yield self.b2i(batch)\n",
    "    \n",
    "    def sentence_batch_generator(self):\n",
    "        all_batch = []\n",
    "        with open(self.path, 'r') as f:\n",
    "            for l in f:\n",
    "                #if len(l) > 2000:\n",
    "                #    continue\n",
    "                all_batch.append(l)\n",
    "        data = self.b2i(all_batch)\n",
    "\n",
    "        n_batch = len(data) / self.batchsize\n",
    "        np.random.shuffle(data)\n",
    "        batchcount = 0\n",
    "        while True:\n",
    "            if batchcount == n_batch:\n",
    "                np.random.shuffle(data)\n",
    "                batchcount = 0\n",
    "            batch = data[batchcount * self.batchsize:(batchcount + 1) * self.batchsize]\n",
    "            batchcount += 1\n",
    "            yield torch.LongTensor(batch)\n",
    "\n",
    "    def negative_batch_generator(self):\n",
    "        all_batch = []\n",
    "        with open(self.path, 'r') as f:\n",
    "            for l in f:\n",
    "                #if len(l) > 2000:\n",
    "                #    continue\n",
    "                all_batch.append(l)\n",
    "        data = self.b2i(all_batch)\n",
    "\n",
    "        data_len = data.shape[0]\n",
    "        dim = data.shape[1]\n",
    "\n",
    "        while True:\n",
    "            indices = np.random.choice(data_len, self.batchsize * self.negsize)\n",
    "            samples = data[indices].reshape(self.batchsize, self.negsize, dim)\n",
    "            yield torch.LongTensor(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import gensim\n",
    "import codecs\n",
    "import tqdm\n",
    "\n",
    "\n",
    "class word2vec:\n",
    "\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.n_words = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        with codecs.open(self.corpus_path, 'r', 'utf-8') as f:\n",
    "            for line in tqdm.tqdm(f, desc='training'):\n",
    "                yield line.split()\n",
    "\n",
    "    def add(self, *words):\n",
    "        for word in words:\n",
    "            if not word in self.w2i:\n",
    "                self.w2i[word] = self.n_words\n",
    "                self.i2w[self.w2i[word]] = word\n",
    "                self.n_words += 1\n",
    "                \n",
    "    def embed(self, model_path, d_embed):\n",
    "        if os.path.isfile(model_path):\n",
    "            model = gensim.models.Word2Vec.load(model_path)\n",
    "        else:\n",
    "            model = gensim.models.Word2Vec(self, \n",
    "                size=d_embed, window=5, min_count=10, workers=8)\n",
    "            model.save(model_path)\n",
    "            model = gensim.models.Word2Vec.load(model_path)\n",
    "\n",
    "        self.i2w, self.w2i = {}, {}\n",
    "        self.add('<pad>')\n",
    "        self.add('<unk>')\n",
    "            \n",
    "        E = []\n",
    "        n = len(model.wv.vocab)\n",
    "        for word in sorted(model.wv.vocab):\n",
    "            j = len(E)\n",
    "            self.i2w[j] = word\n",
    "            self.w2i[word] = j\n",
    "            E.append(list(model.wv[word]))\n",
    "        self.E = np.asarray(E).astype(np.float32)\n",
    "        self.d_embed = d_embed        \n",
    "        return self\n",
    "    \n",
    "    def aspect(self, n_aspects):\n",
    "        self.n_aspects = n_aspects\n",
    "        km = KMeans(n_clusters=n_aspects, random_state=0)\n",
    "        km.fit(self.E)\n",
    "        clusters = km.cluster_centers_\n",
    "\n",
    "        # L2 normalization\n",
    "        norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "        self.T = norm_aspect_matrix.astype(np.float32)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "\n",
    "class wikidata:\n",
    "    \n",
    "    def __init__(self, corpus_path, d_embed=200, n_aspects=14):\n",
    "        self.corpus_path = corpus_path\n",
    "        \n",
    "        self.prep_path = self.corpus_path + '.prep'        \n",
    "        if not os.path.isfile(self.prep_path):\n",
    "            self.preprocess(self.corpus_path, self.prep_path)\n",
    "\n",
    "        self.model_path = self.prep_path + '.w2v'\n",
    "        w2v = word2vec(self.prep_path)\n",
    "        w2v.embed(self.model_path, d_embed)\n",
    "        w2v.aspect(n_aspects)\n",
    "        self.w2v = w2v\n",
    "\n",
    "    def preprocess(self, input_path, output_path):\n",
    "        lmtzr = WordNetLemmatizer()    \n",
    "        stop = stopwords.words('english')\n",
    "        token = CountVectorizer().build_tokenizer()\n",
    "        lc = linecount(input_path)\n",
    "        with open(input_path, 'r') as in_f, open(output_path, 'w') as out_f:\n",
    "            for j, l in tqdm.tqdm(enumerate(in_f), total=lc, desc='preprocessing \"%s\"' % input_path):\n",
    "                tokens = [lmtzr.lemmatize(t) for t in token(l.lower()) if not t in stop]\n",
    "                if len(tokens) > 3:\n",
    "                    out_l = ' '.join(tokens)\n",
    "                    out_f.write(out_l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "epsilon=0.0000001\n",
    "\n",
    "\n",
    "class attention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_embed):\n",
    "        super(attention, self).__init__()\n",
    "        self.M = nn.Linear(d_embed, d_embed)\n",
    "        self.M.weight.data.uniform_(-0.1, 0.1)\n",
    "        #self.M = torch.zeros((d_embed, d_embed), requires_grad=True)\n",
    "        #self.M = self.M.cuda()\n",
    "        #self.M.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, e_i):\n",
    "        y_s = torch.mean(e_i, dim=-1).unsqueeze(1)\n",
    "        #d_i = e_i.t().mm(self.M.mm(y_s))#.tanh()\n",
    "        d_i = e_i.t().mm(self.M.weight.mm(y_s))#.tanh()\n",
    "        a_i = d_i / sum(torch.exp(d_i))\n",
    "        return a_i.squeeze(1)\n",
    "\n",
    "        \n",
    "class abae(nn.Module):\n",
    "    \n",
    "    def __init__(self, w2v, ortho_reg=0.1):\n",
    "        super(abae, self).__init__()\n",
    "        self.ortho_reg = ortho_reg\n",
    "        self.E = nn.Embedding(w2v.n_words, w2v.d_embed)\n",
    "        self.E.weight.data = torch.from_numpy(np.array(w2v.E))        \n",
    "        for param in self.E.parameters():\n",
    "            param.requires_grad = False # freeze layer E\n",
    "\n",
    "        #self.T = nn.Linear(w2v.n_aspects, w2v.d_embed, bias=False)\n",
    "        self.T = nn.Embedding(w2v.n_aspects, w2v.d_embed)\n",
    "        self.T.weight.data = torch.from_numpy(w2v.T)\n",
    "        for param in self.T.parameters():\n",
    "            param.requires_grad = True\n",
    "        #self.T = torch.randn(w2v.n_aspects, w2v.d_embed).float()\n",
    "        #self.T.data = torch.from_numpy(w2v.T)\n",
    "        #self.T.requires_grad = True\n",
    "\n",
    "        self.attention = attention(w2v.d_embed)\n",
    "        self.linear = nn.Linear(w2v.d_embed, w2v.n_aspects)\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, pos, negs):        \n",
    "        e_i = self.E(pos).t()\n",
    "        a_i = self.attention(e_i)\n",
    "        z_s = torch.mv(e_i, a_i)\n",
    "        p_t = self.softmax(self.linear(z_s))\n",
    "        #r_s = self.T.t().mm(p_t.unsqueeze(1)).squeeze(1)\n",
    "        r_s = self.T.weight.t().mm(p_t.unsqueeze(1)).squeeze(1)\n",
    "        e_n = self.E(negs).transpose(1, 2)\n",
    "        z_n = torch.mean(e_n, dim=-1)\n",
    "        loss = max_margin_loss(z_s, r_s, z_n)\n",
    "        return loss + self.regularize()\n",
    "\n",
    "    def regularize(self):\n",
    "        #m = epsilon + torch.norm(self.T, dim=1)\n",
    "        m = epsilon + torch.norm(self.T.weight, dim=1)\n",
    "        #T_n = (self.T.t() / m).t()        \n",
    "        T_n = (self.T.weight.t() / m).t()        \n",
    "        U = T_n.mm(T_n.t()) - torch.eye(T_n.shape[0]).cuda()\n",
    "        return self.ortho_reg * torch.norm(U)\n",
    "\n",
    "\n",
    "def max_margin_loss(z_s, r_s, z_n):\n",
    "    z_s_n = z_s / (epsilon + torch.sqrt(z_s.dot(z_s)))\n",
    "    r_s_n = r_s / (epsilon + torch.sqrt(r_s.dot(r_s)))\n",
    "    pos = z_s_n.dot(r_s_n)\n",
    "    losses = []\n",
    "    for j, n_i in enumerate(z_n):\n",
    "        neg = (n_i / (epsilon + torch.sqrt(n_i.dot(n_i)))).dot(r_s_n)\n",
    "        loss = torch.clamp(torch.ones(1).cuda() - pos + neg, min=0.0)\n",
    "        losses.append(loss)\n",
    "    return torch.stack(losses).sum()\n",
    "\n",
    "\n",
    "def train(d_embed=200, n_aspects=14, epochs=5, lr=0.1, batchsize=64):\n",
    "    #wd = wikidata('./data/wiki_01')\n",
    "    wd = wikidata('./data/restaurant.train.txt', d_embed, n_aspects)\n",
    "    #wd = wikidata('./data/beer.train.txt', d_embed, n_aspects)\n",
    "    \n",
    "    device = 'cuda'\n",
    "\n",
    "    dl = dataloader(wd.w2v.w2i, wd.prep_path, batchsize=batchsize)\n",
    "    sen_gen = dl.sentence_batch_generator()\n",
    "    neg_gen = dl.negative_batch_generator()\n",
    "    \n",
    "    ab = abae(wd.w2v).to(device)\n",
    "    \n",
    "    opt = optim.Adam(ab.parameters(), lr=lr)\n",
    "\n",
    "    n_batches_per_epoch = 100\n",
    "    for e in range(epochs):\n",
    "        with tqdm.trange(n_batches_per_epoch) as pbar:\n",
    "            for b in pbar:\n",
    "                sen_input = next(sen_gen)\n",
    "                neg_input = next(neg_gen)\n",
    "\n",
    "                opt.zero_grad()\n",
    "\n",
    "                losses = []\n",
    "                for pos, negs in zip(sen_input, neg_input):\n",
    "                    #pos.requires_grad = True\n",
    "                    #negs.requires_grad = True\n",
    "                    pos = pos.to(device)\n",
    "                    negs = negs.to(device)\n",
    "                    losses.append(ab(pos, negs))\n",
    "                loss = torch.stack(losses).sum()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                pbar.set_description('e: %d | b: %d | MEAN-LOSS: %f' % (e, b, loss / batchsize))\n",
    "\n",
    "        word_emb = ab.E.weight.cpu().numpy()\n",
    "        word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "        aspect_emb = ab.T.weight.cpu().data.numpy()\n",
    "        aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "\n",
    "        for ind in range(len(aspect_emb)):\n",
    "            desc = aspect_emb[ind]\n",
    "            sims = word_emb.dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "\n",
    "            desc_list = [wd.w2v.i2w[w] for w in ordered_words[:10]]\n",
    "            print('Aspect %d: %s' % (ind, ','.join(desc_list)))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(d_embed=200, epochs=5, lr=0.1, batchsize=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRASH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{'wc %s' % wd.prep_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class dedicated to making a structured dataset a particular data source\n",
    "    raw text -> preprocessing -> splitting\n",
    "    creates a vocab\n",
    "    vocab trains word embeddings\n",
    "\n",
    "class data loader which serves batches of training/evaluation data\n",
    "    requires preprocessed text to serve\n",
    "    requires predetermined vocab\n",
    "    vocab requires word embeddings\n",
    "    \n",
    "class model just the neural network parts\n",
    "\n",
    "class wrap model with interface\n",
    "    training\n",
    "    evaluation\n",
    "    deployment\n",
    "\n",
    "cli script covering interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
