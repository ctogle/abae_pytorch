{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "print(plt.get_backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import mmap\n",
    "import random\n",
    "import contextlib\n",
    "import traceback\n",
    "from abae_pytorch.utils import linecount\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "class dataloader:\n",
    "    \n",
    "    def __init__(self, w2i, path, split=None):\n",
    "        self.w2i = w2i\n",
    "        self.path = path\n",
    "        self.meta = './.' + os.path.basename(self.path) + '.meta.json'\n",
    "        self.split = split if split else {'train': 1.0}\n",
    "       \n",
    "    def sample_splits(self, splits, probs):\n",
    "        r = random.random()\n",
    "        for s, p in zip(splits, np.cumsum(probs)):\n",
    "            if r <= p:\n",
    "                return s\n",
    "    \n",
    "    def __enter__(self):\n",
    "        self.f = open(self.path, 'rb')\n",
    "        self.data = mmap.mmap(self.f.fileno(), 0, access=mmap.ACCESS_COPY)\n",
    "        if os.path.isfile(self.meta):\n",
    "            self.read_meta()\n",
    "        else:\n",
    "            self.offsets = dict((s, []) for s in self.split)\n",
    "            splits, probs = zip(*list(self.split.items()))\n",
    "            desc = 'finding offsets in \"%s\"' % self.path\n",
    "            i = 0\n",
    "            for j, char in enumerate(tqdm.tqdm(self.data, desc=desc)):\n",
    "                if char == b'\\n':\n",
    "                    split = self.sample_splits(splits, probs)\n",
    "                    self.offsets[split].append((i, j))\n",
    "                    i = j + 1\n",
    "            self.linecounts = dict((s, len(self.offsets[s])) for s in self.split)\n",
    "            self.linecount = sum(self.linecounts[s] for s in self.split)\n",
    "            self.write_meta()\n",
    "            \n",
    "            print('offsets for splits:')\n",
    "            for split in self.offsets:\n",
    "                print(' \"%s\" : %d' % (split, len(self.offsets[split])))\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *ags):\n",
    "        if ags[1]:\n",
    "            traceback.print_exception(*ags)\n",
    "        self.f.close()\n",
    "        return True\n",
    "\n",
    "    def write_meta(self):\n",
    "        meta = {\n",
    "            'path': self.path, \n",
    "            'linecount': self.linecount, \n",
    "            'linecounts': self.linecounts, \n",
    "            'offsets': self.offsets, \n",
    "        }\n",
    "        with open(self.meta, 'w') as f:\n",
    "            f.write(json.dumps(meta))\n",
    "    \n",
    "    def read_meta(self):\n",
    "        with open(self.meta, 'r') as f:\n",
    "            meta = json.loads(f.read())\n",
    "        assert(self.path == meta['path'])\n",
    "        self.linecount = meta['linecount']\n",
    "        self.linecounts = meta['linecounts']\n",
    "        self.offsets = meta['offsets']\n",
    "    \n",
    "    def b2i(self, batch):\n",
    "        # use pytorch function for padding if one exists??\n",
    "        batch = [self.data[u:v].decode('utf').split() for u, v in batch]\n",
    "        lengths = [len(l) for l in batch]\n",
    "        index = np.zeros((len(batch), max(lengths)))\n",
    "        w2i = lambda w: (self.w2i[w] if w in self.w2i else self.w2i['<unk>'])\n",
    "        for j, (words, length) in enumerate(zip(batch, lengths)):\n",
    "            index[j, :length] = [w2i(w) for w in words]\n",
    "        return torch.LongTensor(index)\n",
    "\n",
    "    def batch_generator(self, split='train', device='cpu', batchsize=20, negsize=20):\n",
    "        linecount = self.linecounts[split]\n",
    "        batchcount = (linecount // batchsize)\n",
    "        pos_offsets = self.offsets[split][:]\n",
    "        neg_offsets = self.offsets[split][:]\n",
    "        \n",
    "        stime = time.time()\n",
    "        print('shuffling \"%s\" data...' % (split, ), end='\\r')\n",
    "        random.shuffle(pos_offsets)\n",
    "        random.shuffle(neg_offsets)\n",
    "        print('shuffled \"%s\" data! (%0.1f s)' % (split, time.time() - stime))\n",
    "        \n",
    "        batches = 0\n",
    "        while True:\n",
    "            if batches == batchcount:\n",
    "                \n",
    "                print('shuffling data...', end='\\r')\n",
    "                random.shuffle(pos_offsets)\n",
    "                random.shuffle(neg_offsets)\n",
    "                print('shuffled data! (%0.1f s)' % (time.time() - stime))\n",
    "                \n",
    "                batches = 0\n",
    "            pos_batch = pos_offsets[batches * batchsize:(batches + 1) * batchsize]\n",
    "            pos_batch = self.b2i(pos_batch)            \n",
    "            neg_batch = np.random.choice(linecount, batchsize * negsize)\n",
    "            neg_batch = self.b2i([neg_offsets[i] for i in neg_batch])\n",
    "            yield (pos_batch.to(device), neg_batch.to(device).view(batchsize, negsize, -1))\n",
    "            batches += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import gensim\n",
    "import codecs\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "class word2vec:\n",
    "\n",
    "    def __init__(self, corpus_path):\n",
    "        self.corpus_path = corpus_path\n",
    "        self.n_vocab = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        with codecs.open(self.corpus_path, 'r', 'utf-8') as f:\n",
    "            for line in tqdm.tqdm(f, desc='training'):\n",
    "                yield line.split()\n",
    "\n",
    "    def add(self, *words):\n",
    "        for word in words:\n",
    "            if not word in self.w2i:\n",
    "                self.w2i[word] = self.n_vocab\n",
    "                self.i2w[self.w2i[word]] = word\n",
    "                self.n_vocab += 1\n",
    "                \n",
    "    def embed(self, model_path, d_embed, window=5, min_count=10, workers=16):\n",
    "        if os.path.isfile(model_path):\n",
    "            model = gensim.models.Word2Vec.load(model_path)\n",
    "        else:\n",
    "            model = gensim.models.Word2Vec(self, size=d_embed, \n",
    "                window=window, min_count=min_count, workers=workers)\n",
    "            model.save(model_path)\n",
    "            model = gensim.models.Word2Vec.load(model_path)\n",
    "        self.i2w, self.w2i = {}, {}\n",
    "        self.add('<pad>')\n",
    "        self.add('<unk>')\n",
    "        print('loading embeddings...', end='\\r')\n",
    "        E = []\n",
    "        n = len(model.wv.vocab)\n",
    "        for word in sorted(model.wv.vocab):\n",
    "            j = len(E)\n",
    "            self.i2w[j] = word\n",
    "            self.w2i[word] = j\n",
    "            E.append(list(model.wv[word]))\n",
    "        self.E = np.asarray(E)\n",
    "        self.d_embed = d_embed        \n",
    "        print('loaded embeddings!')\n",
    "        return self\n",
    "    \n",
    "    def aspect(self, n_aspects):\n",
    "        self.n_aspects = n_aspects\n",
    "\n",
    "        #self.T = np.random.randn(n_aspects, self.E.shape[1]).astype(np.float32)\n",
    "        #self.T /= np.linalg.norm(self.T, axis=-1, keepdims=True)\n",
    "        #return self        \n",
    "        km = KMeans(n_clusters=n_aspects, random_state=0)\n",
    "\n",
    "        stime = time.time()\n",
    "        print('clustering embeddings...', end='\\r')\n",
    "        km.fit(self.E)\n",
    "        print('clustered embeddings! (%0.1f s)' % (time.time() - stime))\n",
    "        clusters = km.cluster_centers_\n",
    "\n",
    "        # L2 normalization\n",
    "        norm_aspect_matrix = clusters / np.linalg.norm(clusters, axis=-1, keepdims=True)\n",
    "        self.T = norm_aspect_matrix.astype(np.float32)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "from abae_pytorch.utils import linecount\n",
    "\n",
    "\n",
    "class wikidata:\n",
    "    \n",
    "    def __init__(self, corpus_path, d_embed=200, n_aspects=14):\n",
    "        self.corpus_path = corpus_path\n",
    "        \n",
    "        self.prep_path = self.corpus_path + '.prep'        \n",
    "        if not os.path.isfile(self.prep_path):\n",
    "            self.preprocess(self.corpus_path, self.prep_path)\n",
    "\n",
    "        self.model_path = self.prep_path + '.w2v'\n",
    "        w2v = word2vec(self.prep_path)\n",
    "        w2v.embed(self.model_path, d_embed, min_count=100)\n",
    "        w2v.aspect(n_aspects)\n",
    "        self.n_vocab = len(w2v.w2i)\n",
    "        self.w2v = w2v\n",
    "\n",
    "    def preprocess(self, input_path, output_path):\n",
    "        lmtzr = WordNetLemmatizer()    \n",
    "        stop = stopwords.words('english')\n",
    "        token = CountVectorizer().build_tokenizer()\n",
    "        lc = linecount(input_path)\n",
    "        with open(input_path, 'r') as in_f, open(output_path, 'w') as out_f:\n",
    "            for j, l in tqdm.tqdm(enumerate(in_f), total=lc, desc='preprocessing \"%s\"' % input_path):\n",
    "                tokens = [lmtzr.lemmatize(t) for t in token(l.lower()) if not t in stop]\n",
    "                n_tokens = len(tokens)\n",
    "                if len(tokens) > 5 and n_tokens < 100:\n",
    "                    out_l = ' '.join(tokens)\n",
    "                    out_f.write(out_l + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import normalize\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "\n",
    "\n",
    "def max_margin_loss(r_s, z_s, z_n):\n",
    "    device = r_s.device\n",
    "    pos = torch.bmm(z_s.unsqueeze(1), r_s.unsqueeze(2)).squeeze(2)\n",
    "    negs = torch.bmm(z_n, r_s.unsqueeze(2)).squeeze()\n",
    "    J = torch.ones(negs.shape).to(device) - pos.expand(negs.shape) + negs\n",
    "    J = torch.sum(torch.clamp(J, min=0.0))\n",
    "    return J\n",
    "\n",
    "\n",
    "def orthogonal_regularization(T):\n",
    "    T_n = normalize(T, dim=1)\n",
    "    I = torch.eye(T_n.shape[0]).to(T_n.device)\n",
    "    U = torch.norm(T_n.mm(T_n.t()) - I)\n",
    "    return U\n",
    "\n",
    "\n",
    "def sample_aspects(projection, i2w, n=8):\n",
    "    projection = torch.sort(projection, dim=1)\n",
    "    for j, (projs, index) in enumerate(zip(*projection)):\n",
    "        index = index[-n:].detach().cpu().numpy()\n",
    "        words = ', '.join([i2w[i] for i in index])\n",
    "        print('Aspect %2d: %s' % (j + 1, words))\n",
    "\n",
    "\n",
    "def validate(ab, dl, split='val', batchsize=100, negsize=20, device='cuda'):\n",
    "\n",
    "    # figure out how to disable grads??    \n",
    "\n",
    "    batches = dl.batch_generator(split, batchsize=batchsize, negsize=negsize, device=device)\n",
    "    n_batches = (dl.linecounts[split] // batchsize)\n",
    "    with tqdm.tqdm(range(n_batches), total=n_batches, desc='validating') as pbar:\n",
    "        losses = []\n",
    "        for b in pbar:\n",
    "            pos, neg = next(batches)\n",
    "            r_s, z_s, z_n = ab(pos, neg)\n",
    "            J = max_margin_loss(r_s, z_s, z_n)\n",
    "            losses.append(J.item())\n",
    "            x = (b + 1, np.mean(losses) / batchsize)\n",
    "            pbar.set_description('BATCH: %d | MEAN-VAL-LOSS: %0.5f' % x)\n",
    "    return np.mean(losses)\n",
    "\n",
    "        \n",
    "def train(ab, dl, device='cuda', \n",
    "          epochs=5, epochsize=50, initial_lr=0.02, batchsize=100, negsize=20, ortho_reg=0.1):    \n",
    "    train_batches = dl.batch_generator('train', batchsize=batchsize, negsize=negsize, device=device)\n",
    "    i2w = dict((dl.w2i[w], w) for w in dl.w2i)\n",
    "    \n",
    "    validate(ab, dl, 'val', batchsize, negsize, device)\n",
    "    sample_aspects(ab.aspects(), i2w)\n",
    "\n",
    "    mean_losses = []\n",
    "    opt = optim.Adam(ab.parameters(), lr=initial_lr)\n",
    "    for e in range(epochs):\n",
    "        mean_losses.append([])\n",
    "        with tqdm.trange(epochsize) as pbar:\n",
    "            for b in pbar:\n",
    "                pos, neg = next(train_batches)\n",
    "                r_s, z_s, z_n = ab(pos, neg)\n",
    "                J = max_margin_loss(r_s, z_s, z_n)\n",
    "                U = orthogonal_regularization(ab.T.weight)\n",
    "                loss = J + ortho_reg * batchsize * U\n",
    "                opt.zero_grad()\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                mean_losses[-1].append(loss.item())\n",
    "                x = (e + 1, opt.param_groups[0]['lr'], np.mean(mean_losses[-1]) / batchsize)\n",
    "                pbar.set_description('EPOCH: %d | LR: %0.5f | MEAN-TRAIN-LOSS: %0.5f' % x)\n",
    "\n",
    "                if b * batchsize % 100 == 0:\n",
    "                    lr = initial_lr * (1.0 - 1.0 * ((e + 1) * (b + 1)) / (epochs * epochsize))\n",
    "                    for pg in opt.param_groups:\n",
    "                        pg['lr'] = lr\n",
    "        \n",
    "        validate(ab, dl, 'val', batchsize, negsize, device)\n",
    "        sample_aspects(ab.aspects(), i2w)\n",
    "\n",
    "\n",
    "        #all_losses = [x for y in mean_losses for x in y]\n",
    "        all_losses = [np.mean(y) for y in mean_losses]\n",
    "        plt.plot(list(range(len(all_losses))), all_losses, lw=4, marker='s')\n",
    "        plt.semilogy()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from abae_pytorch.model import abae\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "d_embed = 100\n",
    "n_aspects = 20\n",
    "\n",
    "epochs = 20\n",
    "epochsize = 100\n",
    "batchsize = 100\n",
    "negsize = 20\n",
    "initial_lr = 0.001\n",
    "\n",
    "\n",
    "#data = './data/wiki_01'\n",
    "data = './data/beer.train.txt'\n",
    "#data = './data/restaurant.train.txt'\n",
    "split = {'train': 0.8, 'val': 0.1, 'test': 0.1}\n",
    "\n",
    "\n",
    "wd = wikidata(data, d_embed, n_aspects)\n",
    "\n",
    "x = (wd.n_vocab, wd.w2v.d_embed, wd.w2v.n_aspects)\n",
    "print('n_vocab: %d | d_embed: %d | n_aspects: %d' % x)\n",
    "\n",
    "ab = abae(wd.w2v.E, wd.w2v.T).to(device)\n",
    "\n",
    "with dataloader(wd.w2v.w2i, wd.prep_path, split=split) as dl:\n",
    "    train(ab, dl, \n",
    "          device=device,\n",
    "          epochs=epochs, \n",
    "          epochsize=epochsize,\n",
    "          batchsize=batchsize,\n",
    "          negsize=negsize,\n",
    "          initial_lr=initial_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#\n",
    "\n",
    "    offsets partitions in dataloader for splitting?\n",
    "    validation loss measurement\n",
    "    impose maximum vocab size\n",
    "    sentence topic prediction function?\n",
    "\n",
    "    word embeddings trained on partitions too...\n",
    "    optionally use different w2v training corpus\n",
    "    updating loss plot\n",
    "    model saving/loading\n",
    "    inferring n_aspects\n",
    "    cli\n",
    "    break into package\n",
    "    documentation\n",
    "    num tag for preprocessing\n",
    "    downweight specificity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRASH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{'wc %s' % wd.prep_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class dedicated to making a structured dataset a particular data source\n",
    "    raw text -> preprocessing -> splitting\n",
    "    creates a vocab\n",
    "    vocab trains word embeddings\n",
    "\n",
    "class data loader which serves batches of training/evaluation data\n",
    "    requires preprocessed text to serve\n",
    "    requires predetermined vocab\n",
    "    vocab requires word embeddings\n",
    "    \n",
    "class model just the neural network parts\n",
    "\n",
    "class wrap model with interface\n",
    "    training\n",
    "    evaluation\n",
    "    deployment\n",
    "\n",
    "cli script covering interface"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
